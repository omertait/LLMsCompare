# LLM Comparison Tool

## Description
The LLM Comparison Tool is a Python-based application designed to evaluate and compare the performance of various local Large Language Models (LLMs) using the GPT4All framework. It tests LLMs by asking a set of general questions and utilizes Wolfram Alpha to obtain accurate benchmark answers. Additionally, an LLM Judge is employed to assess which model provides the best responses.

## Features
- **Local LLM Comparison**: Uses the GPT4All framework to interact with and compare local LLMs.
- **General Question Testing**: Evaluates LLMs based on their responses to a diverse set of questions.
- **Wolfram Alpha Integration**: Leverages Wolfram Alpha for obtaining factual answers to benchmark LLM responses.
- **LLM Judge**: Employs a separate LLM to judge the quality of answers based on criteria like accuracy and relevance.
- **Performance Metrics**: Measures response time, accuracy, relevance, and coherence of LLM answers.
- **Visualization**: Utilizes libraries like `matplotlib` for visualizing the comparison results.

![image](https://github.com/omertait/LLMsCompare/assets/120312961/d90f2a3a-7f94-49ea-a610-99234db7dd30)

![image](https://github.com/omertait/LLMsCompare/assets/120312961/a150c594-6019-4ffb-82e9-939044dfbb60)

![image](https://github.com/omertait/LLMsCompare/assets/120312961/d39564a0-8acc-4a29-ab59-a7712a0e7e00)

![image](https://github.com/omertait/LLMsCompare/assets/120312961/9e8e931c-2ea3-411f-a82b-0f51fa481b6e)

![image](https://github.com/omertait/LLMsCompare/assets/120312961/71791599-e4a5-411e-8052-f8baee7567c2)

