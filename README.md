# LLM Comparison Tool

## Description
The LLM Comparison Tool is a Python-based application designed to evaluate and compare the performance of various local Large Language Models (LLMs) using the GPT4All framework. It tests LLMs by asking a set of general questions and utilizes Wolfram Alpha to obtain accurate benchmark answers. Additionally, an LLM Judge is employed to assess which model provides the best responses.

## Features
- **Local LLM Comparison**: Uses the GPT4All framework to interact with and compare local LLMs.
- **General Question Testing**: Evaluates LLMs based on their responses to a diverse set of questions.
- **Wolfram Alpha Integration**: Leverages Wolfram Alpha for obtaining factual answers to benchmark LLM responses.
- **LLM Judge**: Employs a separate LLM to judge the quality of answers based on criteria like accuracy and relevance.
- **Performance Metrics**: Measures response time, accuracy, relevance, and coherence of LLM answers.
- **Visualization**: Utilizes libraries like `matplotlib` for visualizing the comparison results.
